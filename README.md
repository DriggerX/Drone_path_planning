
# Q-Learning-Based Path Planning

---

## ðŸ§  Project Overview
This project demonstrates how **Q-learning**, a reinforcement learning algorithm, can be used to teach an agent (robot/drone) to find the shortest path in both **2D** and **3D** grid environments with obstacles. The implementation includes:

- A grid-based 2D environment with static obstacles.
- An extended 3D environment with multi-storey building blocks.
- A modular Python codebase to simulate, train, and visualize the agent's learning process.

---

## ðŸ—ºï¸ Problem Statement
Given a start point and a goal point in a grid with obstacles, the objective is to train an agent to learn the optimal policy (shortest collision-free path) using **model-free reinforcement learning (Q-learning)**.

---

## âš™ï¸ Q-Learning Algorithm

### ðŸ”¢ Core Update Rule:
```
Q(s, a) = Q(s, a) + Î± [r + Î³ max Q(s', a') - Q(s, a)]
```

### Parameters:

| Parameter              | Description                                    |
| ---------------------- | ---------------------------------------------- |
| `Î±` (Learning rate)    | How fast the Q-table updates                   |
| `Î³` (Discount rate)    | Future reward importance                       |
| `Îµ` (Exploration rate) | Trade-off between exploration and exploitation |

### ðŸ§­ Exploration Policy
An Îµ-greedy strategy is used with **exponential decay**:
```
Îµ = Îµ_{min} + (Îµ_{max} - Îµ_{min}) * exp(-decay * episode)
```

---

## ðŸ§© Environment Setup

### âœ… 2D Environment
- Grid: 10x10
- Obstacles: Predefined static blocks
- Actions: [up, down, left, right]
- State: (x, y)
- Q-table Shape: [10, 10, 4]

### âœ… 3D Environment
- Grid: 10x10x10
- Obstacles: Buildings with variable heights
- Actions: [up, down, left, right, up_z, down_z]
- State: (x, y, z)
- Q-table Shape: [10, 10, 10, 6]

---

## ðŸ”„ Workflow & Flowchart

### **Flowchart:**
```
   +------------------+
   | Initialize Q-Table|
   +------------------+
             |
             v
   +------------------+
   | Set environment  |
   +------------------+
             |
             v
   +--------------------------+
   | For each episode:        |
   |  - Choose action (explore/exploit) |
   |  - Move to next state    |
   |  - Get reward            |
   |  - Update Q-value        |
   |  - Decay Îµ              |
   +--------------------------+
             |
             v
   +------------------+
   | Use learned Q-Table |
   | for path testing   |
   +------------------+
             |
             v
   +----------------------+
   | Visualize path & grid |
   +----------------------+
```

---

## ðŸ Code Structure
```
.
â”œâ”€â”€ rl_path_optimized.py      # 2D Q-learning implementation
â”œâ”€â”€ rl_3dpath.py              # 3D Q-learning implementation
â”œâ”€â”€ images/
â”‚   â””â”€â”€ environment_diagram.jpg  # Visual map layout
â”œâ”€â”€ plots/                    # Generated result plots
â”œâ”€â”€ README.md                 # Project overview (this file)
â””â”€â”€ wiki/                     # Optional extended documentation
```

---

## ðŸ“Š Results

### 2D Environment:
- Agent learns to move around complex obstacles to reach the goal.
- Optimal path plotted using heatmap.

### 3D Environment:
- Agent learns vertical and horizontal maneuvers.
- 3D bar plots visualize both buildings and path.

---

## ðŸ“ˆ Visualization Samples
- `matplotlib` used for 3D path and bar charts.
- `seaborn.heatmap` for 2D path visualization.

---

## ðŸš€ Future Work
- Dynamic obstacle support.
- Continuous space learning via Deep Q Networks (DQN).
- Real drone integration using ROS or PX4 flight controller.

---

## ðŸ™Œ Credits
- Developed by: [Your Name]
- Tools used: Python, NumPy, Matplotlib, Seaborn
- Special thanks to OpenAIâ€™s support tools for documentation.

---

## ðŸ“Ž Related Files
- `rl_path_optimized.py`
- `rl_3dpath.py`
- 2D grid design (image)
- Q-table output plots

---

> For any questions, open an [Issue](https://github.com/your-repo/issues) or reach out to the maintainer.
